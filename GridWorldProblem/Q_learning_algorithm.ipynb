{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-learning algorithm for Grid World problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Enviroment\n",
    "Model represeting enviroment of Grid World, where agent need to find the best path to reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Action(Enum):\n",
    "    UP = 'UP'\n",
    "    DOWN = \"DOWN\"\n",
    "    LEFT = \"LEFT\"\n",
    "    RIGHT = \"RIGHT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorldEnv:\n",
    "    CONST_PENALTY = -0.1   \n",
    "\n",
    "    states_actions = {\n",
    "        ((0,0), Action.DOWN) : [(1., ((0,1), CONST_PENALTY))],\n",
    "        ((0,0), Action.RIGHT) : [(1., ((1,0), CONST_PENALTY))],\n",
    "        ((1,0), Action.LEFT) : [(1., ((0,0), CONST_PENALTY))],\n",
    "        ((1,0), Action.RIGHT) : [(1., ((2,0), CONST_PENALTY))],\n",
    "        ((2,0), Action.LEFT) : [(1., ((1,0), CONST_PENALTY))],\n",
    "        ((2,0), Action.RIGHT) : [(.1, ((3,0), 1.)), (.9, ((1,0), CONST_PENALTY))],\n",
    "        ((2,0), Action.DOWN) : [(1., ((2,1), CONST_PENALTY))],\n",
    "        ((0,1), Action.DOWN) : [(1., ((0,2), CONST_PENALTY))],\n",
    "        ((0,1), Action.UP) : [(1., ((0,0), CONST_PENALTY))],\n",
    "        ((2,1), Action.UP) : [(.7, ((2,0), CONST_PENALTY)), (.3, ((3,1), -1.))],\n",
    "        ((2,1), Action.DOWN) : [(1., ((2,1), CONST_PENALTY))],\n",
    "        ((2,1), Action.RIGHT) : [(1., ((3,1), -1.))],\n",
    "        ((0,2), Action.UP) : [(1., ((0,1), CONST_PENALTY))],\n",
    "        ((0,2), Action.RIGHT) : [(1., ((1,2), CONST_PENALTY))],\n",
    "        ((1,2), Action.RIGHT) : [(1., ((2,2), CONST_PENALTY))],\n",
    "        ((1,2), Action.LEFT) : [(1., ((0,2), CONST_PENALTY))],\n",
    "        ((2,2), Action.RIGHT) : [(1., ((3,2), CONST_PENALTY))],\n",
    "        ((2,2), Action.LEFT) : [(1., ((1,2), CONST_PENALTY))],\n",
    "        ((2,2), Action.UP) : [(1., ((2,1), CONST_PENALTY))],\n",
    "        ((3,2), Action.LEFT) : [(1., ((2,2), CONST_PENALTY))],\n",
    "        ((3,2), Action.UP) : [(1., ((3,1), -1.))],\n",
    "    }\n",
    "\n",
    "    actions = {\n",
    "        (0,0) : {Action.DOWN, Action.RIGHT},\n",
    "        (1,0) : {Action.LEFT, Action.RIGHT},\n",
    "        (2,0) : {Action.LEFT, Action.RIGHT, Action.DOWN},\n",
    "        (3,0) : {},\n",
    "        (0,1) : {Action.UP, Action.DOWN},\n",
    "        (2,1) : {Action.UP, Action.DOWN, Action.RIGHT},\n",
    "        (3,1) : {},\n",
    "        (0,2) : {Action.UP, Action.RIGHT},\n",
    "        (1,2) : {Action.RIGHT, Action.LEFT},\n",
    "        (2,2) : {Action.RIGHT, Action.LEFT, Action.UP},\n",
    "        (3,2) : {Action.LEFT, Action.UP}\n",
    "    }\n",
    "\n",
    "    def env_return(self, state, action):\n",
    "        probabilities, return_value = zip(*self.states_actions[(state, action)])\n",
    "        return random.choices(return_value, weights=probabilities, k=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActionValue:    \n",
    "    def __init__(self, env):    \n",
    "        self.Q = {}\n",
    "        for (state, action) in env.states_actions:\n",
    "            self.Q[(state, action)] = (random.random() * 2) - 1\n",
    "    \n",
    "    def get_max_action(self, state):\n",
    "        max_a = None\n",
    "        max_value = float('-inf')\n",
    "        for (s,a) in self.Q:\n",
    "            if s == state and self.Q[(s,a)] > max_value:\n",
    "                max_a = a\n",
    "                max_value = self.Q[(s,a)]\n",
    "        return max_a, max_value\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy:\n",
    "    def __init__(self, env: GridWorldEnv, Q: ActionValue, epsilon):\n",
    "        self.env = env\n",
    "        self.Q = Q\n",
    "        self.epsilon = epsilon\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        if self.epsilon > random.random():\n",
    "            return random.choice(list(self.env.actions[state]))\n",
    "        else:\n",
    "            return self.Q.get_max_action(state)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GridWorldEnv()\n",
    "Q = ActionValue(env)\n",
    "policy = Policy(env=env, Q=Q, epsilon=.1)\n",
    "alpha = .01\n",
    "gamma = .1   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(1000):\n",
    "    current_state = (0,2)\n",
    "    while env.actions[current_state] != {}:\n",
    "        action = policy.get_action(current_state)\n",
    "        next_state, reward = env.env_return(current_state, action)\n",
    "        cur_Q = Q.Q[current_state, action]\n",
    "        Q.Q[current_state, action] = cur_Q + alpha*(reward + gamma*Q.get_max_action(next_state)[1] - cur_Q)\n",
    "        current_state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{((0, 0), <Action.DOWN: 'DOWN'>): -0.11107780587452647,\n",
       " ((0, 0), <Action.RIGHT: 'RIGHT'>): -0.1111110960222626,\n",
       " ((1, 0), <Action.LEFT: 'LEFT'>): -0.11110773409695733,\n",
       " ((1, 0), <Action.RIGHT: 'RIGHT'>): -0.11111111067428155,\n",
       " ((2, 0), <Action.LEFT: 'LEFT'>): -0.11111095025184597,\n",
       " ((2, 0), <Action.RIGHT: 'RIGHT'>): nan,\n",
       " ((2, 0), <Action.DOWN: 'DOWN'>): -0.30056889730333153,\n",
       " ((0, 1), <Action.DOWN: 'DOWN'>): -0.11110417722399714,\n",
       " ((0, 1), <Action.UP: 'UP'>): -0.1111093137229863,\n",
       " ((2, 1), <Action.UP: 'UP'>): nan,\n",
       " ((2, 1), <Action.DOWN: 'DOWN'>): -0.11111111111111036,\n",
       " ((2, 1), <Action.RIGHT: 'RIGHT'>): nan,\n",
       " ((0, 2), <Action.UP: 'UP'>): -0.11109769736066707,\n",
       " ((0, 2), <Action.RIGHT: 'RIGHT'>): -0.11109774850173088,\n",
       " ((1, 2), <Action.RIGHT: 'RIGHT'>): -0.11111100366677246,\n",
       " ((1, 2), <Action.LEFT: 'LEFT'>): -0.12106963510458993,\n",
       " ((2, 2), <Action.RIGHT: 'RIGHT'>): -0.11111111111111188,\n",
       " ((2, 2), <Action.LEFT: 'LEFT'>): -0.11321595386420673,\n",
       " ((2, 2), <Action.UP: 'UP'>): -0.11829721334041612,\n",
       " ((3, 2), <Action.LEFT: 'LEFT'>): -0.11111111111111188,\n",
       " ((3, 2), <Action.UP: 'UP'>): nan}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Q.Q)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
